{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soufianelotfi-lab/Classification_Formes_Ondes_6G/blob/main/Models1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qtt5eweL7DR"
      },
      "source": [
        "**`I. PREPARATION DE DATA POUR LE MODELE :`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8kxo7NH9uq7"
      },
      "source": [
        "\n",
        "**`1. Préparation de l’environnement et des librairies`**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5VEn_xrw92I"
      },
      "outputs": [],
      "source": [
        "!pip install scipy tqdm -q\n",
        "!pip install torch torchvision torchaudio -q\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device utilisé :\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cyExB6YJNGW"
      },
      "source": [
        "**`2. Upload des dataset`**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cette cellule permet d’accéder au dataset stocké sur Google Drive et de le rendre disponible dans l’environnement Google Colab."
      ],
      "metadata": {
        "id": "3h5nwDnLPFPQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpM83gvhDt4C"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Adapter le chemin si le fichier est stocké ailleurs dans le Drive\n",
        "files.download(\"/content/drive/MyDrive/5ème/Projet_techno/data_set_40k.mat\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcbGj7XZJX74"
      },
      "source": [
        "**`3. Chargement des fichiers MATLAB (.mat)`**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cette cellule permet de charger le fichier de données MATLAB (.mat) stocké sur Google Drive.\n",
        "Le chemin du fichier est défini puis le contenu est importé en mémoire à l’aide de scipy.io.loadmat, ce qui rend les signaux accessibles en Python sous forme de dictionnaire"
      ],
      "metadata": {
        "id": "sPJIy2OcQGsQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JKz8r-cOJYJs"
      },
      "outputs": [],
      "source": [
        "# Chemin vers le fichier de données MATLAB\n",
        "signals_path = \"/content/drive/MyDrive/5ème/Projet_techno/data_set_40k.mat\"\n",
        "\n",
        "print(\"Chemin signaux :\", signals_path)\n",
        "mat_signals = sio.loadmat(signals_path)\n",
        "print(\"Fichier des signaux chargé.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKEu4n3kRL3Q"
      },
      "source": [
        "**`4. Exploration du contenu des fichiers .mat`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7BfSNgNJy7n"
      },
      "source": [
        "- Pour savoir le nom du fichier qui est telechargé (ici on trouve que c'est data_set) apres globals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmzEs3iLKwJd"
      },
      "outputs": [],
      "source": [
        "print(\"Variables dans le fichier des signaux :\")\n",
        "print(mat_signals.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEQSr89sVQyZ"
      },
      "source": [
        "**`5. Extraction brute de X et y depuis les fichiers MATLAB`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzmAdpjhSPc-"
      },
      "outputs": [],
      "source": [
        "X = mat_signals[\"data_set\"]\n",
        "print(\"Shape brute X :\", X.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tucBNB3WL4jS"
      },
      "source": [
        "**`5.a. Tracer un signal`**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cette cellule permet de visualiser un signal brut extrait du dataset afin d’avoir un aperçu temporel des données."
      ],
      "metadata": {
        "id": "iWFpC4vAREIo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNr9PmXiEXSf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sélection de l'indice du signal à visualiser\n",
        "index = 20000\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(X[index])\n",
        "plt.title(f\"Signal numéro {index}\")\n",
        "plt.xlabel(\"Échantillons\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFNdoM77TR8H"
      },
      "source": [
        "**`5.b. Tracer des échantillons: `**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cette cellule permet de visualiser les échantillons discrets d’un signal sélectionné dans le dataset."
      ],
      "metadata": {
        "id": "qjAiPYNPR-Yk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y2SP5_qSdtg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Sélection du signal et de la plage d'échantillons\n",
        "index = 33000 # Indice du signal sélectionné dans le dataset (0 à 9999 : classe 1, 10000 à 19999 : classe 2, 20000 à 29999 : classe 3, 30000 à 39999 : classe 4)\n",
        "start = 0 # Indice de début des échantillons à afficher\n",
        "end = 4096  # Indice de fin des échantillons à afficher\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.stem(X[index][start:end])\n",
        "plt.title(\"Classe 4 \")\n",
        "plt.xlabel(\"Index des échantillons\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv7OOgWnWFf5"
      },
      "source": [
        "**`6. Inspection des dimensions de X`**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cette cellule permet de vérifier le type et les dimensions du tableau contenant les données.\n",
        "Cela garantit que la structure de X est conforme avant de passer aux étapes de traitement ou d’apprentissage."
      ],
      "metadata": {
        "id": "skiu8FbmTewo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5KF1QIrThNJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Type X :\", type(X))\n",
        "print(\"Shape X :\", np.shape(X))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8I1B7hKbbMl"
      },
      "source": [
        "**`6. Distribution des échantillons (avant reshape)`**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cette cellule permet d’analyser la distribution globale des valeurs I/Q avant toute mise en forme des données. Les échantillons sont aplatis afin d’observer la répartition statistique des amplitudes sur l’ensemble du dataset. L’analyse montre que les valeurs sont majoritairement concentrées autour de zéro, sans présence de valeurs extrêmes significatives, ce qui confirme une distribution stable avant le reshape et l’apprentissage."
      ],
      "metadata": {
        "id": "WPznrHIWUQqI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnbKKdFmTg8O"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.hist(X.flatten(), bins=200)\n",
        "plt.title(\"Histogramme des valeurs avant reshape\")\n",
        "plt.xlabel(\"Valeurs\")\n",
        "plt.ylabel(\"Fréquence\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jV2W0KWUqMP"
      },
      "source": [
        "**`7. Changement de la structure de X : `**\n",
        "\n",
        "\n",
        "*   La structure initiale de X n’est pas directement compatible avec PyTorch, qui attend des données sous la forme (nombre de signaux, nombre de canaux, nombre d’échantillons).\n",
        "Les valeurs I et Q étant concaténées dans une seule dimension, la matrice est reformée afin de séparer explicitement les deux canaux.\n",
        "*   Pour en faire, nous avons restruture le formt de X :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h53IBxwOJ5Un"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Affichage de la forme initiale des données\n",
        "print(\"Shape X avant traitement :\", X.shape)   # (40000, 4096)\n",
        "\n",
        "# Nombre total de signaux\n",
        "N = X.shape[0]   # 40000 signaux\n",
        "\n",
        "# Nombre total de valeurs par signal (I + Q concaténés)\n",
        "total_features = X.shape[1]  # 4096 valeurs par signal\n",
        "\n",
        "# Vérification que la dimension est divisible par 2 (I et Q)\n",
        "if total_features % 2 != 0:\n",
        "    raise ValueError(\"Le nombre de features par signal n'est pas divisible par 2, ce n'est pas cohérent avec 2 canaux I/Q.\")\n",
        "\n",
        "# Nombre d'échantillons par canal\n",
        "L = total_features // 2   # 2048 échantillons par canal\n",
        "print(\"Nombre de canaux :\", 2)\n",
        "print(\"Longueur par canal :\", L)\n",
        "\n",
        "# On reforme X en (N, 2, L) = (40000, 2, 2048)\n",
        "X = X.reshape(N, 2, L)\n",
        "print(\"le Shape X après reshape :\", X.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PHgrmOyMUgd"
      },
      "source": [
        "**- FFT**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cette cellule permet de transformer les signaux du domaine temporel vers le domaine fréquentiel en calculant la FFT et sa magnitude, tout en conservant la structure des données.\n",
        "Elle est à exécuter uniquement si l’analyse ou l’apprentissage est réalisé dans le domaine fréquentiel.\n",
        "Dans le cas contraire, si l’on travaille directement dans le domaine temporel, cette cellule peut être ignorée."
      ],
      "metadata": {
        "id": "v-DsKc03WAkv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHtpmDoTLuHk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.fft\n",
        "import numpy as np\n",
        "\n",
        "print(\"Shape original X:\", X.shape)\n",
        "\n",
        "\n",
        "# 0) Si X est un numpy ndarray, convertir en Tensor\n",
        "if isinstance(X, np.ndarray):\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    print(\"X converti en Tensor :\", X.shape)\n",
        "\n",
        "# 1) Calcul FFT magnitude\n",
        "def compute_fft_keep_shape(X):\n",
        "    \"\"\"\n",
        "    X : (batch, 2, L)\n",
        "    Retour : (batch, 1, L)\n",
        "    \"\"\"\n",
        "    I = X[:, 0, :]              # (batch, L)\n",
        "    Q = X[:, 1, :]              # (batch, L)\n",
        "\n",
        "    # Signal complexe\n",
        "    x_complex = I + 1j * Q      # complexe = compatible pytorch\n",
        "\n",
        "    # FFT\n",
        "    X_fft = torch.fft.fft(x_complex)\n",
        "\n",
        "    # Magnitude\n",
        "    X_mag = torch.abs(X_fft)    # (batch, L)\n",
        "\n",
        "    # Normalisation par signal\n",
        "    X_mag = X_mag / (X_mag.max(dim=1, keepdim=True)[0] + 1e-6)\n",
        "\n",
        "    # CNN 1D attend (batch, channels, length)\n",
        "    X_mag = X_mag.unsqueeze(1)  # (batch, 1, L)\n",
        "\n",
        "    return X_mag.float()\n",
        "\n",
        "X = compute_fft_keep_shape(X)\n",
        "\n",
        "print(\"Shape X après FFT :\", X.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Choisir l'indice du signal\n",
        "index = 3001\n",
        "\n",
        "# Extraire le spectre fréquentiel (magnitude FFT)\n",
        "fft_signal = X[index, 0].cpu().numpy()\n",
        "\n",
        "# Affichage\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(fft_signal)\n",
        "plt.xlabel(\"Indice fréquentiel\")\n",
        "plt.ylabel(\"Amplitude normalisée\")\n",
        "plt.title(f\"Signal {index} après FFT (domaine fréquentiel)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ssqoXwg2CAVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZS9Vl8wbyTl"
      },
      "source": [
        "**`8. Distribution des échantillons I/Q (apres reshape)`**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cette cellule permet de visualiser la distribution des composantes I et Q lorsque l’on travaille dans le domaine temporel.\n",
        "Les échantillons des deux canaux sont séparés puis représentés sous forme d’histogrammes afin d’analyser leur répartition statistique.\n",
        "Cette visualisation permet de comparer le comportement des canaux I et Q avant toute transformation fréquentielle."
      ],
      "metadata": {
        "id": "k2wGU2kyWkVV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImjQGo0sSzTQ"
      },
      "outputs": [],
      "source": [
        "# À utiliser uniquement si l'analyse est réalisée dans le domaine temporel\n",
        "I = X[:,0,:].flatten()\n",
        "Q = X[:,1,:].flatten()\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(I, bins=200)\n",
        "plt.title(\"Histogramme du canal I\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(Q, bins=200)\n",
        "plt.title(\"Histogramme du canal Q\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9cdMZBbW0qQ"
      },
      "source": [
        "**`Tracer des echantillons de I et Q :`**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dans cette étape, les échantillons des composantes I et Q d’un même signal sont tracés séparément dans le domaine temporel."
      ],
      "metadata": {
        "id": "XFq9HLVbXLK9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HZqPHhJVIsV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "index = 0     # choisis le signal que vous voulez tracer\n",
        "start = 0     # début\n",
        "end = 4096     # fin (par exemple 200 premiers échantillons)\n",
        "\n",
        "I = X[index, 0, start:end]   # canal I\n",
        "Q = X[index, 1, start:end]   # canal Q\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "# Signal I\n",
        "plt.subplot(1,2,1)\n",
        "plt.stem(I)\n",
        "plt.title(f\"Canal I — échantillons {start}:{end}\")\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Signal Q\n",
        "plt.subplot(1,2,2)\n",
        "plt.stem(Q)\n",
        "plt.title(f\"Canal Q — échantillons {start}:{end}\")\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR9b4c-JLCN2"
      },
      "source": [
        "**`8. Creation des labels :`**\n",
        "\n",
        "- Dans cette étape, les labels ne sont pas directement extraits du fichier de données mais générés manuellement.\n",
        "Un vecteur de labels est créé en supposant une répartition équilibrée des signaux, avec le même nombre d’échantillons pour chaque classe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntnpQWLBLCfZ"
      },
      "outputs": [],
      "source": [
        "n_classes = 4\n",
        "n_per_class = 10000\n",
        "Y = np.repeat(np.arange(n_classes), n_per_class)\n",
        "\n",
        "print(\"Premiers labels :\", Y[:20]) #Pour afficher les 20 premiers labels pour vérifier le début du vecteur\n",
        "print(\"Derniers labels :\", Y[-20000:]) #Pour afficher les 20000 derniers labels pour vérifier la fin du vecteur\n",
        "print(\"Shape y :\", Y.shape)\n",
        "print(\"Valeurs uniques :\", np.unique(Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuyYg-NWWhCn"
      },
      "outputs": [],
      "source": [
        "print(\"min =\", X.min())\n",
        "print(\"max =\", X.max())\n",
        "print(\"mean =\", X.mean())\n",
        "print(\"std =\", X.std())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIilGOXaXFQV"
      },
      "source": [
        "**`9. Creation du Dataset:`**     \n",
        "  - La classe WaveformDataset permet non seulement de structurer les 40000 signaux I/Q dans un format compatible avec PyTorch, mais aussi d’associer automatiquement chaque signal à son label grâce à la méthode __getitem__, qui renvoie pour un indice donné le couple (signal_i, label_i).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JOg94YEWmPh"
      },
      "outputs": [],
      "source": [
        "class WaveformDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        # X : numpy (N, 2, 2304)\n",
        "        # Y : numpy (N,)\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.Y = torch.tensor(Y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Nombre total de signaux\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Un signal+son label\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "# Création du dataset complet\n",
        "dataset = WaveformDataset(X, Y) # juste pour s'assurer que la classe fait son travail, fonctionne correctement sinon c'est facultatif (c'est pas le data qu'on va utiliser)\n",
        "print(\"Taille du dataset :\", len(dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77U4Zvjymknb"
      },
      "source": [
        "\n",
        "**`10. Division de Dataset pour un partie d'entrainemenet et une partie de test`**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hyGpjoz-mHkt"
      },
      "outputs": [],
      "source": [
        "# Première division entrainement + test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X,\n",
        "    Y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,     # reproductibilité\n",
        "    stratify=Y,          # même proportion de chaque classe, que ca soit dns la partie d'entrainement ou test, les 4 classes doivent etre équilbré de la meme quantité\n",
        "    shuffle=True         # mélanger les signaux\n",
        ")\n",
        "\n",
        "print(\"Taille X_train :\", X_train.shape)\n",
        "print(\"Taille X_test  :\", X_test.shape)\n",
        "print(\"Taille Y_train :\", Y_train.shape)\n",
        "print(\"Taille Y_test  :\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gm1aKO6Nja_"
      },
      "outputs": [],
      "source": [
        "# Deuxième division: créer un ensemble de validation à partir de l'ensemble d'entraînement (validation + entrainement)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(\n",
        "    X_train,\n",
        "    Y_train,\n",
        "    test_size=0.125,\n",
        "    random_state=42,\n",
        "    stratify=Y_train,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(\"Taille X_train :\", X_train.shape)\n",
        "print(\"Taille X_val   :\", X_val.shape)\n",
        "print(\"Taille X_test  :\", X_test.shape)\n",
        "print(\"Taille Y_train :\", Y_train.shape)\n",
        "print(\"Taille Y_val   :\", Y_val.shape)\n",
        "print(\"Taille Y_test  :\", Y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSmDLIMhkq6P"
      },
      "source": [
        "**`- Sassurer que la division est equlibrée`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdiQzI04eLdU"
      },
      "outputs": [],
      "source": [
        "print(\"Train counts:\", np.unique(Y_train, return_counts=True))\n",
        "print(\"Val counts  :\", np.unique(Y_val, return_counts=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cette cellule permet de vérifier que les classes sont équilibrées dans chaque ensemble de données (entraînement, validation et test).\n",
        "- La distribution des labels est représentée sous forme d’histogrammes afin de s’assurer que chaque classe est correctement représentée après le découpage du dataset.\n",
        "- Cette vérification permet d’éviter tout biais lié à un déséquilibre des classes lors de l’apprentissage et de l’évaluation."
      ],
      "metadata": {
        "id": "_pOy_RE-Y1kH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2OIISMikiSQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_distribution(y, title):\n",
        "    classes, counts = np.unique(y, return_counts=True)\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.bar(classes, counts)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Classes\")\n",
        "    plt.ylabel(\"Nombre d'exemples\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
        "    plt.show()\n",
        "\n",
        "plot_distribution(Y_train, \"Distribution Train\")\n",
        "plot_distribution(Y_val, \"Distribution Validation\")\n",
        "plot_distribution(Y_test, \"Distribution Test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRmaHMi4kkgd"
      },
      "source": [
        "**`- Normalisation des donnés (facultatif)`**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cette cellule permet de normaliser les données à partir des statistiques de l’ensemble d’entraînement.\n",
        "- La moyenne et l’écart-type sont calculés puis utilisés pour normaliser les ensembles d’entraînement, de validation et de test de manière cohérente.\n",
        "- Cette étape est facultative, mais elle peut améliorer la stabilité et la convergence de l’apprentissage."
      ],
      "metadata": {
        "id": "g1J0v6UQZHcH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnY_7zNHsLE-"
      },
      "outputs": [],
      "source": [
        "mean = X_train.mean(axis=(0, 2), keepdims=True)   # shape (1, 2, 1)\n",
        "std  = X_train.std(axis=(0, 2), keepdims=True) + 1e-8\n",
        "\n",
        "X_train_norm = (X_train - mean) / std\n",
        "X_val_norm   = (X_val   - mean) / std\n",
        "X_test_norm  = (X_test  - mean) / std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MYAO6-Nonzu"
      },
      "source": [
        "**`11. Creation de la data sous forme (signaux+labls):`**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Dans cette étape, les données sont regroupées sous la forme (signal, label) à l’aide de la classe WaveformDataset qu'on a crée avant.\n",
        "- Les ensembles d’entraînement, de validation et de test sont ainsi préparés à partir des données qu'on vient de séparer.\n",
        "- Cette structuration permet une utilisation directe avec les outils PyTorch (DataLoader, entraînement, évaluation).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhvS5mEsom4O"
      },
      "outputs": [],
      "source": [
        "train_dataset = WaveformDataset(X_train_norm, Y_train)\n",
        "val_dataset   = WaveformDataset(X_val_norm,   Y_val)\n",
        "test_dataset  = WaveformDataset(X_test_norm,  Y_test)\n",
        "\n",
        "print(\"Taille train_dataset :\", len(train_dataset))\n",
        "print(\"Taille val_dataset   :\", len(val_dataset))\n",
        "print(\"Taille test_dataset  :\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uilLKyLKt65-"
      },
      "source": [
        "**`12. Dataloader :`**\n",
        "- Nous avons utilisé le DataLoader de PyTorch pour organiser les données en mini-lots (batch), ce qui permet d’envoyer les signaux au modèle par petites séquences durant l’entraînement. trois DataLoaders ont été créés : train_loader et val_loader pour entraîner le modèle et test_loader pour évaluer ses performances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYW6e6F0uDAT"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "#La partie d'entrainement\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle=True    # mélange les échantillons à chaque epoch pour améliorer la généralisation et éviter que le modèle apprenne l'ordre des données et réduire le risque d'overfitting.\n",
        ")\n",
        "\n",
        "# Loader de validation\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False  # pas besoin de mélanger ici\n",
        ")\n",
        "\n",
        "#La partie de test\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False   # éviter que le modèle apprenne l'ordre des données\n",
        ")\n",
        "\n",
        "# Petit test pour vérifier qu'un batch ressemble à ce qu'on veut (optionnel juste pour vérification)\n",
        "for X_batch, Y_batch in train_loader:\n",
        "    print(\"Shape X_batch :\", X_batch.shape)  # attendu : (32, 2, 2304)\n",
        "    print(\"Shape y_batch :\", Y_batch.shape)  # attendu : (32,)\n",
        "    print(\"Quelques labels :\", Y_batch[:10])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRNtV4uoMZ-R"
      },
      "source": [
        "**`II. CREATION DE MODELE`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65EcfBPgVR7w"
      },
      "source": [
        "**`1. Définiton du modèle :`**\n",
        "\n",
        "- Les deux premiers modèles sont conçus pour un traitement dans le domaine temporel.\n",
        "- Le premier modèle, plus profond et comportant un grand nombre de couches, offre une forte capacité de représentation mais présente un surapprentissage.\n",
        "- Le second modèle constitue une version optimisée et plus compacte, permettant\n",
        "de limiter un peu ce surapprentissage (mais qui persiste toujours).\n",
        "- Enfin, le troisième modèle est utilisé lorsque l’analyse est réalisée dans le domaine fréquentiel, après transformation des signaux par FFT.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`1.1 Models pour D.temporel`**"
      ],
      "metadata": {
        "id": "CnSmcmNBDuVg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ-iWLBEarem"
      },
      "source": [
        "**Modele 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L7y9YkmHFqQ"
      },
      "outputs": [],
      "source": [
        "# Définition de la classe du modèle de Deep Learning\n",
        "class WaveformClassifier_3(nn.Module):\n",
        "\n",
        "    # Constructeur du modèle\n",
        "    def __init__(self):\n",
        "        # Appel du constructeur de la classe parente nn.Module\n",
        "        super(WaveformClassifier_3, self).__init__()\n",
        "\n",
        "        # Définition du bloc CNN pour l'extraction de caractéristiques locales\n",
        "        self.cnn = nn.Sequential(\n",
        "            # Première convolution : passage de 2 canaux (I/Q) à 32 filtres\n",
        "            nn.Conv1d(2, 32, kernel_size=9, padding=4),\n",
        "            # Normalisation pour stabiliser l'entraînement\n",
        "            nn.BatchNorm1d(32),\n",
        "            # Fonction d'activation non linéaire\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Deuxième convolution : augmentation du nombre de filtres\n",
        "            nn.Conv1d(32, 64, kernel_size=7, padding=3),\n",
        "            # Normalisation\n",
        "            nn.BatchNorm1d(64),\n",
        "            # Activation\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Troisième convolution : représentation plus abstraite\n",
        "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
        "            # Normalisation\n",
        "            nn.BatchNorm1d(128),\n",
        "            # Activation\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Réduction de la dimension temporelle\n",
        "            nn.MaxPool1d(2)\n",
        "            # Dropout possible pour réduire le surapprentissage (désactivé ici)\n",
        "            # nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # Définition du LSTM pour modéliser les dépendances temporelles\n",
        "        self.lstm = nn.LSTM(\n",
        "            # Nombre de features en entrée du LSTM (sortie du CNN)\n",
        "            input_size=128,\n",
        "            # Taille de l'état caché\n",
        "            hidden_size=32,\n",
        "            # Nombre de couches LSTM\n",
        "            num_layers=1,\n",
        "            # LSTM bidirectionnel (avant + arrière)\n",
        "            bidirectional=True\n",
        "            # Dropout possible entre couches LSTM (désactivé ici)\n",
        "            # dropout=0.3\n",
        "        )\n",
        "\n",
        "        # Définition de la couche fully connected pour la classification finale\n",
        "        self.fc = nn.Linear(32 * 2, 4 ) # 32*2 car LSTM bidirectionnel (dans 32 (Taille de l'état caché) qui represente la sortie du LSTM)\n",
        "\n",
        "    # Définition du passage avant (forward)\n",
        "    def forward(self, x):\n",
        "        # x est de forme (batch_size, 2, 2304)\n",
        "\n",
        "        # Passage des données dans le CNN\n",
        "        x = self.cnn(x)               # (batch_size, 128, L')\n",
        "\n",
        "        # Transposition pour correspondre au format attendu par le LSTM\n",
        "        x = x.transpose(1, 2)         # (batch_size, L', 128)\n",
        "\n",
        "        # Passage dans le LSTM\n",
        "        lstm_out, _ = self.lstm(x)    # (batch_size, L', 64)\n",
        "\n",
        "        # Agrégation temporelle par moyenne sur l'axe du temps\n",
        "        last_time_step = lstm_out.mean(dim=1)\n",
        "\n",
        "        # Passage dans la couche dense pour obtenir les logits\n",
        "        out = self.fc(last_time_step)\n",
        "\n",
        "        # Retour de la sortie du modèle\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**- Optimisation de l’Architecture du Modèle**"
      ],
      "metadata": {
        "id": "XHGEB34w_JoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WaveformClassifier_2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WaveformClassifier_2, self).__init__()\n",
        "        # La couche CNN\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(2, 32, kernel_size=9, padding=4),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(32, 48, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(48),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout(0.3)  # nouveau : 30% des neurones désactivés aléatoirement\n",
        "        )\n",
        "        #La couche LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size= 48,\n",
        "            hidden_size= 32, #changr 64 par 32\n",
        "            num_layers=1, #j'ai ajouté une couche -----------------------------------------------------------------------------------------------\n",
        "            batch_first=True, #le batch est en premier dans les dimensions : (batch, seq, features)\n",
        "            bidirectional= True, # LSTM dans les deux sens (avant + arrière)\n",
        "            #dropout=0.3\n",
        "        )\n",
        "        #La couche dense\n",
        "        self.fc = nn.Linear(32*2 , 4)  # 64*2 car bidirectionnel,128 sorties, 4 classes à prédire\n",
        "        #self.fc = nn.Linear(32, 4)  # 64*2 car bidirectionnel\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x : (batch_size, 2, 2304)\n",
        "        x = self.cnn(x)             # (batch, 64, L)\n",
        "        x = x.transpose(1, 2)       #  (batch, L, 64)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x)  # (batch, L, 128)\n",
        "        #last_time_step = lstm_out[:, -1, :]  # (batch, 128)\n",
        "        last_time_step = lstm_out.mean(dim=1)\n",
        "\n",
        "        out = self.fc(last_time_step)\n",
        "        return out"
      ],
      "metadata": {
        "id": "9wY_OBfosM9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`1.2 Model pour D.fréquentiel`**"
      ],
      "metadata": {
        "id": "bilc9RRWD2mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WaveformClassifier_DF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WaveformClassifier_DF, self).__init__()\n",
        "        # La couche CNN\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=9, padding=4), # un seul canal d'entrée\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(32, 64, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(64,128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout(0.3)  # nouveau : 30% des neurones désactivés aléatoirement\n",
        "\n",
        "        )\n",
        "        #La couche LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size= 128,\n",
        "            hidden_size= 64,\n",
        "            num_layers=2, # Le nombre de couche LSTM\n",
        "            batch_first=True, #le batch est en premier dans les dimensions : (batch, seq, features)\n",
        "            bidirectional= True, # LSTM dans les deux sens (avant + arrière)\n",
        "            dropout= 0.3 # si nous travaillons avec plus d'une seule couche\n",
        "        )\n",
        "        #La couche dense\n",
        "        self.fc = nn.Linear(64 * 2, 4)  #  *2 car bidirectionnel, 4 classes à prédire\n",
        "        #self.fc = nn.Linear(32, 4)  # 64*2 car bidirectionnel\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        last_time_step = lstm_out.mean(dim=1)\n",
        "\n",
        "        out = self.fc(last_time_step)\n",
        "        return out"
      ],
      "metadata": {
        "id": "8ItgtUseD7ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDCHv2tIVJ4B"
      },
      "source": [
        "**`2. Créer le modèle + choisir le device`**\n",
        "\n",
        "\n",
        "* Cette instruction détecte automatiquement si un GPU (CUDA) est disponible et choisit le meilleur matériel pour exécuter le modèle, sinon elle utilise le CPU. Ensuite, le modèle WaveformClassifier est créé et transféré sur ce device afin d’accélérer l’entraînement et garantir que toutes les opérations se font sur le même support.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si_b1jviU_3n"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device utilisé :\", device)\n",
        "\n",
        "model = WaveformClassifier_DF().to(device) #mettre le model voulu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAIKHGA32v2l"
      },
      "source": [
        "**`- Initialisation des poids`**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cette cellule définit une initialisation personnalisée des poids du modèle afin d’améliorer la stabilité et la convergence de l’entraînement.\n",
        "- Des méthodes adaptées sont utilisées selon le type de couche (CNN, couche dense ou LSTM).\n",
        "- Cette étape permet de partir d’un état initial plus favorable que l’initialisation par défaut."
      ],
      "metadata": {
        "id": "_6pqL55_gHcV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9esDImbR2nhz"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    elif isinstance(m, nn.LSTM):\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'weight_ih' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'weight_hh' in name:\n",
        "                nn.init.orthogonal_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7_69Il0mG3s"
      },
      "source": [
        "**`3. Définir la loss et l’optimiseur`**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cette étape applique l’initialisation des poids définie précédemment à l’ensemble du modèle.\n",
        "- La fonction de perte et l’optimiseur sont ensuite configurés afin d’entraîner le réseau pour notre problème de classification multi-classes.\n"
      ],
      "metadata": {
        "id": "7XWjOG1AgTX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzRm7H1DVigc"
      },
      "outputs": [],
      "source": [
        "# Application de l'initialisation personnalisée des poids au modèle\n",
        "model.apply(init_weights)\n",
        "\n",
        "# Définition de la fonction de perte pour une classification à 4 classes\n",
        "criterion = nn.CrossEntropyLoss()                 # pour la classification en 4 classes\n",
        "\n",
        "# Définition de l'optimiseur Adam pour l'entraînement du modèle\n",
        "optimizer = torch.optim.Adam(model.parameters(),  # tous les poids du modèle\n",
        "                             lr=0.0001 # Taux d'apprentissage\n",
        "                             )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sthDMo1aHyyA"
      },
      "source": [
        "**`4. Fonction d'entraînement`**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cette fonction réalise une époque complète d’entraînement du modèle.\n",
        "Pour chaque batch, elle effectue la propagation avant, le calcul de la perte, la rétropropagation et la mise à jour des poids.\n",
        "Elle retourne ensuite la perte moyenne et la précision sur l’ensemble de l’époque."
      ],
      "metadata": {
        "id": "62jv4kfhg24g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quOwW6FoIEtS"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "   # Parcours des batches de données\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        # 1) Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2) Propagation avant (Forward)\n",
        "        outputs = model(X_batch)\n",
        "\n",
        "        # 3) Calcul de la fonction de perte (loss)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        # 4) Rétropropagation (Backprop)\n",
        "        loss.backward()\n",
        "\n",
        "        # 5) Update poids\n",
        "        optimizer.step()\n",
        "\n",
        "        # Stats\n",
        "        running_loss += loss.item() * X_batch.size(0)\n",
        "        _, preds = torch.max(outputs, 1) # il permt de detrminer les classes pédites dans tout le batch en se basant sur le maximum\n",
        "        correct += (preds == y_batch).sum().item() #il compare les predictions au vraies classes\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWzsh3jbu_WR"
      },
      "source": [
        "**`5. Fonction d'evaluation`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N42JiW1MPzYl"
      },
      "outputs": [],
      "source": [
        "def eval_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()  # pas d’apprentissage\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # pas de calcul de gradient\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "            running_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqQcIavdNo--"
      },
      "source": [
        "**`6. Fonction test / validation`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4eS_UoJNuCR"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "            running_loss += loss.item() * X_batch.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qyo-rDFwPZq"
      },
      "source": [
        "**`III. Apprentissage et evaluation du modèle :`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggn9NptsOPiF"
      },
      "source": [
        "**`7. La boucle d'apprentissage`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXmfXbZiVowo"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "\n",
        "num_epochs = 50\n",
        "best_val_loss = float('inf')   # pour suivre le meilleur modèle\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Entraînement pour une epoch\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_acc = eval_one_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "    # Sauvegarde des courbes\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracy.append(train_acc)\n",
        "    val_accuracy.append(val_acc)\n",
        "\n",
        "    # Affichage\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "    # Sauvegarde automatique du meilleur modèle\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(\">>> Meilleur modèle sauvegardé (val_loss améliorée)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wJhUSoFjJuS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_loss_curves(train_losses, val_losses):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    # Trouver le meilleur epoch (min de la val_loss)\n",
        "    best_epoch = np.argmin(val_losses) + 1\n",
        "    best_val = val_losses[best_epoch - 1]\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
        "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
        "\n",
        "    # Point rouge du meilleur epoch\n",
        "    plt.scatter(best_epoch, best_val, color='red', s=60)\n",
        "\n",
        "    # Ligne verticale\n",
        "    plt.axvline(best_epoch, color='red', linestyle='--', alpha=0.7,\n",
        "                label=f\"Best Epoch = {best_epoch}\")\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Courbe de Loss (Train vs Validation)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_accuracy_curves(train_accuracies, val_accuracies):\n",
        "    epochs = range(1, len(train_accuracies) + 1)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [a * 100 for a in train_accuracies], label=\"Train Acc (%)\")\n",
        "    plt.plot(epochs, [a * 100 for a in val_accuracies], label=\"Val Acc (%)\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.title(\"Courbe d’Accuracy (Train vs Validation)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmnSPSyljOo0"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(train_losses, val_losses)\n",
        "plot_accuracy_curves(train_accuracy, val_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK406GmQbxE2"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUYJoIkU8bW-"
      },
      "source": [
        "**`8. Phase de test`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ebNfrM_110N"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc*100:.2f}%\")\n",
        "\n",
        "html_table = f\"\"\"\n",
        "<table border=\"1\" style=\"\n",
        "    border-collapse: collapse;\n",
        "    text-align: center;\n",
        "    width: 60%;\n",
        "    font-size: 20px;\n",
        "    margin-top: 20px;\n",
        "\">\n",
        "    <tr style=\"background-color: #f0f0f0; font-weight: bold;\">\n",
        "        <th style=\"padding: 10px;\">Metric</th>\n",
        "        <th style=\"padding: 10px;\">Value</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td style=\"padding: 12px;\">Test Loss</td>\n",
        "        <td style=\"padding: 12px;\">{test_loss:.4f}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td style=\"padding: 12px;\">Test Accuracy</td>\n",
        "        <td style=\"padding: 12px;\">{test_acc*100:.2f}%</td>\n",
        "    </tr>\n",
        "</table>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(html_table))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlHyZsGiBT2K"
      },
      "source": [
        "**`9. Matrice de confusion`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAEQ7cq6_Op9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "\n",
        "y_true_list = []\n",
        "y_pred_list = []\n",
        "\n",
        "# 1) On parcourt tout le test_loader\n",
        "with torch.no_grad():\n",
        "    for X, y in test_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        outputs = model(X)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        # On stocke labels réels et prédits\n",
        "        y_true_list.extend(y.cpu().numpy())\n",
        "        y_pred_list.extend(predicted.cpu().numpy())\n",
        "\n",
        "# 2) Calcul de la matrice de confusion\n",
        "cm = confusion_matrix(y_true_list, y_pred_list)\n",
        "\n",
        "print(\"Matrice de confusion :\")\n",
        "print(cm)\n",
        "\n",
        "# 3) Affichage graphique\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(cm, cmap=\"Greens\")\n",
        "plt.title(\"Matrice de Confusion\")\n",
        "plt.xlabel(\"Prédictions\")\n",
        "plt.ylabel(\"Vérités\")\n",
        "plt.colorbar()\n",
        "\n",
        "# Afficher les valeurs dans les cases\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== TABLEAU PRECISION / RECALL / F1 EN % =====\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:   # ou val_loader\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# ---- Métriques par classe (en %) ----\n",
        "precision_pc = precision_score(all_labels, all_preds, average=None) * 100\n",
        "recall_pc    = recall_score(all_labels, all_preds, average=None) * 100\n",
        "f1_pc        = f1_score(all_labels, all_preds, average=None) * 100\n",
        "support_pc   = pd.Series(all_labels).value_counts().sort_index().values\n",
        "\n",
        "# ---- Tableau ----\n",
        "df_metrics = pd.DataFrame({\n",
        "    \"Precision (%)\": precision_pc,\n",
        "    \"Recall (%)\": recall_pc,\n",
        "    \"F1-score (%)\": f1_pc,\n",
        "    \"Support\": support_pc\n",
        "})\n",
        "\n",
        "# ---- Moyennes ----\n",
        "df_metrics.loc[\"Macro avg\"] = [\n",
        "    precision_score(all_labels, all_preds, average=\"macro\") * 100,\n",
        "    recall_score(all_labels, all_preds, average=\"macro\") * 100,\n",
        "    f1_score(all_labels, all_preds, average=\"macro\") * 100,\n",
        "    sum(support_pc)\n",
        "]\n",
        "\n",
        "df_metrics.loc[\"Weighted avg\"] = [\n",
        "    precision_score(all_labels, all_preds, average=\"weighted\") * 100,\n",
        "    recall_score(all_labels, all_preds, average=\"weighted\") * 100,\n",
        "    f1_score(all_labels, all_preds, average=\"weighted\") * 100,\n",
        "    sum(support_pc)\n",
        "]\n",
        "\n",
        "# ---- Arrondi pour affichage ----\n",
        "df_metrics = df_metrics.round(2)\n",
        "\n",
        "df_metrics\n"
      ],
      "metadata": {
        "id": "DIIrAgEJ90xz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      },
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}